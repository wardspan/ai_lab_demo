services:
  mock-llm:
    build:
      context: .
      dockerfile: Dockerfile.mock
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ./jailbreak_demo:/app/jailbreak_demo
      - ./harness:/app/harness
      - ./rag_demo:/app/rag_demo
      - ./rag_redact:/app/rag_redact
      - ./poisoning_demo:/app/poisoning_demo
      - ./tools:/app/tools
      - ./logs:/app/logs
      - ./.env:/app/.env:ro
    command: ["uvicorn", "jailbreak_demo.server:app", "--host", "0.0.0.0", "--port", "8000"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://127.0.0.1:8000/healthz"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s

  rag:
    build:
      context: .
      dockerfile: Dockerfile.rag
    env_file:
      - .env
    volumes:
      - ./rag_demo:/app/rag_demo
    command: ["bash", "-lc", "python rag_demo/build_docs.py && python rag_demo/rag_demo.py --defended"]
    depends_on:
      mock-llm:
        condition: service_started

  controller_api:
    build:
      context: .
      dockerfile: controller_api/Dockerfile
    container_name: controller_api
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - LAB_ENDPOINT=http://mock-llm:8000/complete
      - JAILBREAK_API=http://mock-llm:8000/complete
      - PYTHONPATH=/workspace
    volumes:
      - ./:/workspace
    working_dir: /workspace
    ports:
      - "5055:5055"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - mock-llm

  webui:
    build:
      context: ./webui
      dockerfile: Dockerfile
    container_name: lab_webui
    environment:
      - VITE_API_BASE=http://localhost:5055/api
    ports:
      - "5173:5173"
    volumes:
      - ./webui:/app
      - /app/node_modules
    depends_on:
      - controller_api

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    profiles:
      - ollama-docker
    # Note: Host must accept the model license when pulling models
    # Use 'docker compose --profile ollama-docker up -d' to include this service
    # Or set OLLAMA_MODE=local to use your local installation instead

volumes:
  ollama-data:
